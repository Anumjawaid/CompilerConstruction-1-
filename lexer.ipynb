{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DT'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyword = {\"if\":\"if\",\"switch\":\"switch\",\"case\":\"case\",\"break\":\"break\",\"default\":\"default\",\"iterate\":\"iterate\",\n",
    "          \"in\":\"in\",\"range\":\"range\",\"while\":\"while\",\"elif\":\"elif\",\"else\":\"else\",\"class\":\"class\",\"return\":\"return\",\n",
    "          \"num\":\"DT\",\"dec\":\"DT\",\"alpha\":\"DT\",\"string\":\"DT\",\"flag\":\"DT\",\"1\":\"flag\",\"0\":\"flag\",\"func\":\"func\",\"safe\":\"safe\",\n",
    "          \"personal\":\"personal\",\"available\":\"available\",\"locked\":\"locked\",\"banned\":\"banned\",\"implicit\":\"implicit\", \n",
    "           \"stack\":\"stack\",\"overwritten\":\"overwritten\"}\n",
    "keyword[\"num\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "opr = {\"+\":\"PM\",\"-\":\"PM\",\"*\":\"MDM\",\"/\":\"MDM\",\"%\":\"MDM\",\"!\":\"NOT\",\"~\":\"NOT\",\"&&\":\"&&\",\"||\":\"||\",\"band\":\"band\",\"bor\":\"bor\",\n",
    "      \"<\":\"RO\",\">\":\"RO\",\"<=\":\"RO\",\">=\":\"RO\",\"!=\":\"RO\",\"==\":\"RO\",\"=\":\"Assign\",\"<<\":\"shiftOp\",\">>\":\"shiftOp\",\"+=\":\"AssignOp\",\n",
    "       \"*=\":\"AssignOp\",\"/=\":\"AssignOp\",\"%==\":\"AssignOp\",\"<<=\":\"AssignOp\",\">>=\":\"AssignOp\",\"++\":\"Inc_Dec\",\"--\":\"Inc_Dec\",\n",
    "      \"**\":\"Power\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "punc = {\",\":\",\" , \";\":\";\" , \":\":\":\" , \".\":\".\" , \"{\":\"{\" , \"}\":\"}\" , \"(\":\"(\" , \")\":\")\" ,\n",
    "        \"[\":\"[\" , \"]\":\"]\" }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "#tokenclass\n",
    "class token:\n",
    "    def __init__(self,text):\n",
    "        #text in temp\n",
    "        self.text=text\n",
    "    def tokenizer(self):\n",
    "        \n",
    "        if self.text in keyword:\n",
    "            \n",
    "            for key,value in keyword.items():\n",
    "            \n",
    "                if self.text ==key:#y is class part and a is value part\n",
    "                    if value==self.text:\n",
    "                        y=value\n",
    "                        a=\" \"\n",
    "                    else:\n",
    "                        \n",
    "                        y=value\n",
    "                        a = self.text\n",
    "        elif self.text in punc:\n",
    "            \n",
    "             for key,value in punc.items():\n",
    "                    \n",
    "                    if self.text ==key:\n",
    "                        if value==self.text:\n",
    "                                y=value\n",
    "                                a=\" \"\n",
    "                        else:\n",
    "                        \n",
    "                            y=value\n",
    "                            a = self.text\n",
    "                      \n",
    "        elif self.text in opr:\n",
    "            \n",
    "            for key,value in opr.items():\n",
    "                  if self.text ==key:\n",
    "                        if value==self.text:\n",
    "                                y=value\n",
    "                                a=\" \"\n",
    "                        else:\n",
    "                        \n",
    "                            y=value\n",
    "                            a = self.text\n",
    "                      \n",
    "                      \n",
    "        elif self.text == '\\n':\n",
    "             \n",
    "            y =self.text\n",
    "            a =\" \"\n",
    "        elif re.match(r'^[+-]?[0-9]+$',self.text):\n",
    "            a=self.text\n",
    "            y=\"INTEGER CONSTANT\"\n",
    "        elif re.match(r'^[+-]?[0-9]*(.)[0-9]+$',self.text):\n",
    "            a=self.text\n",
    "            y=\"FLOAT CONSTANT\"\n",
    "        elif re.match(r'^_+[a-zA-Z_0-9]*[A-Za-z0-9]$|^[A-Za-z]+[A-Za-z_0-9]*[A-Za-z0-9]$|^[A-Za-z]+$',self.text):\n",
    "            a=self.text\n",
    "            y=\"IDENTIFIER\" \n",
    "            \n",
    "        elif re.match(r\"^'[a-zA-Z0-9]'$|^'[!@#$%^&*()-=+{}|;:<>,.?/']'$|^'\\\\[\\'\\\"\\\\]'$|^'\\\\[ntrafb0]'$\",self.text):\n",
    "            a=self.text\n",
    "            y=\"CHAR CONSTANT\"\n",
    "        elif re.match(\"^\\\"([a-zA-Z0-9]|(\\\\\\\\\\\\\\\\)|[!@#$%^&*()-=+{}|;:<>,.?/']|\\\\[|\\\\]|_|\\\\\\\\[nrtfab0]|\\\\\\\\\\\"|\\\\\\\\')*\\\"$\",self.text):\n",
    "            a=self.text\n",
    "            y=\"STRING CONSTANT\"\n",
    "            \n",
    "        else:\n",
    "            y=\"Invalid Token\"\n",
    "            a=self.text\n",
    "    \n",
    "        \n",
    "        return a,y\n",
    "    \n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"SAF.txt\", 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "with open(\"SAF1.txt\", 'w') as f:\n",
    "    for line in lines:\n",
    "        # Keep the Shebang line\n",
    "        if line[0:2] == \"#!\":\n",
    "            f.writelines(line)\n",
    "        # Also keep existing empty lines\n",
    "        elif not line.strip():\n",
    "            f.writelines(line)\n",
    "        # But remove comments from other lines\n",
    "        else:\n",
    "            line = line.split('#')\n",
    "            stripped_string = line[0].rstrip()\n",
    "            # Write the line only if the comment was after the code.\n",
    "            # Discard lines that only contain comments.\n",
    "            if stripped_string:\n",
    "                f.writelines(stripped_string)\n",
    "                f.writelines('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'IDENTIFIER', 1]\n",
      "['+=', 'AssignOp', 1]\n",
      "['b', 'IDENTIFIER', 1]\n",
      "[' ', '\\n', 1]\n",
      "['', 'Invalid Token', 2]\n",
      "['65.75', 'FLOAT CONSTANT', 2]\n",
      "[' ', '\\n', 2]\n",
      "['sdfg', 'IDENTIFIER', 3]\n",
      "[' ', '\\n', 3]\n",
      "['sdfgh', 'IDENTIFIER', 4]\n",
      "[['a', 'IDENTIFIER', 1], ['+=', 'AssignOp', 1], ['b', 'IDENTIFIER', 1], [' ', '\\n', 1], ['', 'Invalid Token', 2], ['65.75', 'FLOAT CONSTANT', 2], [' ', '\\n', 2], ['sdfg', 'IDENTIFIER', 3], [' ', '\\n', 3], ['sdfgh', 'IDENTIFIER', 4]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#openning file\n",
    "with open(\"SAF1.txt\",\"r\")as f:\n",
    "    file=f.read()\n",
    "#empty token list\n",
    "all_tokens=[]\n",
    "#the one works correctly\n",
    "\n",
    "eq=[\"=\"]\n",
    "op=['+','-','*','<','>','/','|','band','bor','!']\n",
    "do=['++','--','+=','*=','/=']\n",
    "sym=[';','#','\\t']\n",
    "punct=[',',':','}','{','[',']','(',')']\n",
    "result = []\n",
    "all_tokens = []\n",
    "le=''\n",
    "sp=' '\n",
    "c = 1\n",
    "no=['1','2','3','4','5','6','7','8','9','0']\n",
    "key=op+sym+eq+punct\n",
    "string=\"a+=b\\n 65.75\\nsdfg\\nsdfgh  \"\n",
    "for a,i in enumerate(string):\n",
    "    if i == '\\n':\n",
    "        le+=i #visa to the token\n",
    "        toke = token(le)\n",
    "        n=toke.tokenizer()\n",
    "        for t in n:\n",
    "            result.append(t)\n",
    "        result.append(c)\n",
    "        all_tokens.append(result)\n",
    "        print(result)\n",
    "        c+=1\n",
    "        result=[]\n",
    "        le=le.strip()#for newline\n",
    "    try:\n",
    "        if i !=sp and i not in sym:#checking and appending into le                  eq=[\"=\"]\n",
    "\n",
    "            if i in op and string[a+1] in eq :\n",
    "                #double assignment\n",
    "                le=i+string[a+1]\n",
    "        \n",
    "            elif i == string[a+1] and i != string[a-1]:\n",
    "                \n",
    "                le=i+string[a+1]\n",
    "                if string[a+1]==string[a-1]:\n",
    "                    pass\n",
    "                \n",
    "            elif i in eq and string[a-1] in op :\n",
    "                pass\n",
    "           # elif i in op and string[a+1]in op:\n",
    "            #     le=i+string[a+1] \n",
    "            else:\n",
    "                 le+=i\n",
    "        \n",
    "    except IndexError:\n",
    "        print(\"END OF FILE\")\n",
    "        \n",
    " \n",
    "   \n",
    "    if(a+1<len(string)):\n",
    "        if string[a+1] == sp or string[a+1] in key or le in key  or string[a+1] == '\\n' :#checking the next word(breaker)\n",
    "            if le != '':\n",
    "                \n",
    "                #token for/n\n",
    "                if '\\n' in le:\n",
    "                    le=le.strip()\n",
    "                    #print(le.replace('\\n',''))\n",
    "                    toke = token(le)\n",
    "                    n=toke.tokenizer()\n",
    "                    for t in n:\n",
    "                        result.append(t)\n",
    "                    result.append(c)    \n",
    "                    all_tokens.append(result)\n",
    "                    print(result)\n",
    "                    result = []             \n",
    "                    le=''\n",
    "                else:  \n",
    "                    toke = token(le)\n",
    "                    n=toke.tokenizer()\n",
    "                    for t in n:\n",
    "                        result.append(t)\n",
    "                    result.append(c)     \n",
    "                    all_tokens.append(result)\n",
    "                    print(result)\n",
    "                    result = []               \n",
    "                    le=''\n",
    "                    \n",
    "            elif le == '' or string[a+1]==sp:\n",
    "                pass\n",
    "    \n",
    "    \n",
    "\n",
    "print(all_tokens)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['void', 'IDENTIFIER', 1]\n",
      "['main', 'IDENTIFIER', 1]\n",
      "[' ', '(', 1]\n",
      "[' ', '(', 1]\n",
      "['a', 'IDENTIFIER', 1]\n",
      "['<<', 'shiftOp', 1]\n",
      "['<', 'RO', 1]\n",
      "['b', 'IDENTIFIER', 1]\n",
      "['==', 'RO', 1]\n",
      "['=', 'Assign', 1]\n",
      "['=', 'Assign', 1]\n",
      "['c', 'IDENTIFIER', 1]\n",
      "['!=', 'RO', 1]\n",
      "['d', 'IDENTIFIER', 1]\n",
      "[' ', ')', 1]\n",
      "['\\\\n', 'Invalid Token', 1]\n",
      "[' ', '\\n', 1]\n",
      "['', 'Invalid Token', 2]\n",
      "[' ', '{', 2]\n",
      "[' ', '{', 2]\n",
      "[' ', 'while', 2]\n",
      "[' ', '(', 2]\n",
      "['a', 'IDENTIFIER', 2]\n",
      "['.b', 'Invalid Token', 2]\n",
      "['.c', 'Invalid Token', 2]\n",
      "['.d', 'Invalid Token', 2]\n",
      "['!=', 'RO', 2]\n",
      "['75', 'INTEGER CONSTANT', 2]\n",
      "['+=', 'AssignOp', 2]\n",
      "['65$$b', 'Invalid Token', 2]\n",
      "['.x', 'Invalid Token', 2]\n",
      "['.7xd', 'Invalid Token', 2]\n",
      "[' ', '\\n', 2]\n",
      "['a', 'IDENTIFIER', 3]\n",
      "['<<', 'shiftOp', 3]\n",
      "['<', 'RO', 3]\n",
      "['<=', 'RO', 3]\n",
      "['=', 'Assign', 3]\n",
      "['b', 'IDENTIFIER', 3]\n",
      "['!!', 'Invalid Token', 3]\n",
      "['!=', 'RO', 3]\n",
      "['=', 'Assign', 3]\n",
      "['65.75.1b5c', 'Invalid Token', 3]\n",
      "['.5bc66.786', 'Invalid Token', 3]\n",
      "[' ', '\\n', 3]\n",
      "['a', 'IDENTIFIER', 4]\n",
      "['+=', 'AssignOp', 4]\n",
      "['**', 'Power', 4]\n",
      "['*', 'MDM', 4]\n",
      "[' ', '\\n', 4]\n",
      "['a', 'IDENTIFIER', 5]\n",
      "['++', 'Inc_Dec', 5]\n",
      "[' ', '\\n', 5]\n",
      "['string', 'DT', 6]\n",
      "['str', 'IDENTIFIER', 6]\n",
      "['+=', 'AssignOp', 6]\n",
      "['\"abcd', 'Invalid Token', 6]\n",
      "['*=', 'AssignOp', 6]\n",
      "['=', 'Assign', 6]\n",
      "['65\\\\\\\\\"abcd', 'Invalid Token', 6]\n",
      "['+=', 'AssignOp', 6]\n",
      "['56\"', 'Invalid Token', 6]\n",
      "['+', 'PM', 6]\n",
      "['5', 'INTEGER CONSTANT', 6]\n",
      "[' ', '\\n', 6]\n",
      "['alpha', 'DT', 7]\n",
      "['ch', 'IDENTIFIER', 7]\n",
      "[\"'\\\\\\\\n'\", 'Invalid Token', 7]\n",
      "['+', 'PM', 7]\n",
      "[\"'\", 'Invalid Token', 7]\n",
      "['+=', 'AssignOp', 7]\n",
      "['\"\\\\n\\'', 'Invalid Token', 7]\n",
      "['+', 'PM', 7]\n",
      "[\"'''r\", 'Invalid Token', 7]\n",
      "[' ', '\\n', 7]\n",
      "['char', 'IDENTIFIER', 8]\n",
      "['\"a', 'Invalid Token', 8]\n",
      "['+', 'PM', 8]\n",
      "['b\"', 'Invalid Token', 8]\n",
      "[' ', '\\n', 8]\n",
      "end of fileii\n",
      "[['void', 'IDENTIFIER', 1], ['main', 'IDENTIFIER', 1], [' ', '(', 1], [' ', '(', 1], ['a', 'IDENTIFIER', 1], ['<<', 'shiftOp', 1], ['<', 'RO', 1], ['b', 'IDENTIFIER', 1], ['==', 'RO', 1], ['=', 'Assign', 1], ['=', 'Assign', 1], ['c', 'IDENTIFIER', 1], ['!=', 'RO', 1], ['d', 'IDENTIFIER', 1], [' ', ')', 1], ['\\\\n', 'Invalid Token', 1], [' ', '\\n', 1], ['', 'Invalid Token', 2], [' ', '{', 2], [' ', '{', 2], [' ', 'while', 2], [' ', '(', 2], ['a', 'IDENTIFIER', 2], ['.b', 'Invalid Token', 2], ['.c', 'Invalid Token', 2], ['.d', 'Invalid Token', 2], ['!=', 'RO', 2], ['75', 'INTEGER CONSTANT', 2], ['+=', 'AssignOp', 2], ['65$$b', 'Invalid Token', 2], ['.x', 'Invalid Token', 2], ['.7xd', 'Invalid Token', 2], [' ', '\\n', 2], ['a', 'IDENTIFIER', 3], ['<<', 'shiftOp', 3], ['<', 'RO', 3], ['<=', 'RO', 3], ['=', 'Assign', 3], ['b', 'IDENTIFIER', 3], ['!!', 'Invalid Token', 3], ['!=', 'RO', 3], ['=', 'Assign', 3], ['65.75.1b5c', 'Invalid Token', 3], ['.5bc66.786', 'Invalid Token', 3], [' ', '\\n', 3], ['a', 'IDENTIFIER', 4], ['+=', 'AssignOp', 4], ['**', 'Power', 4], ['*', 'MDM', 4], [' ', '\\n', 4], ['a', 'IDENTIFIER', 5], ['++', 'Inc_Dec', 5], [' ', '\\n', 5], ['string', 'DT', 6], ['str', 'IDENTIFIER', 6], ['+=', 'AssignOp', 6], ['\"abcd', 'Invalid Token', 6], ['*=', 'AssignOp', 6], ['=', 'Assign', 6], ['65\\\\\\\\\"abcd', 'Invalid Token', 6], ['+=', 'AssignOp', 6], ['56\"', 'Invalid Token', 6], ['+', 'PM', 6], ['5', 'INTEGER CONSTANT', 6], [' ', '\\n', 6], ['alpha', 'DT', 7], ['ch', 'IDENTIFIER', 7], [\"'\\\\\\\\n'\", 'Invalid Token', 7], ['+', 'PM', 7], [\"'\", 'Invalid Token', 7], ['+=', 'AssignOp', 7], ['\"\\\\n\\'', 'Invalid Token', 7], ['+', 'PM', 7], [\"'''r\", 'Invalid Token', 7], [' ', '\\n', 7], ['char', 'IDENTIFIER', 8], ['\"a', 'Invalid Token', 8], ['+', 'PM', 8], ['b\"', 'Invalid Token', 8], [' ', '\\n', 8]]\n"
     ]
    }
   ],
   "source": [
    "with open(\"SAF1.txt\",\"r\")as f:\n",
    "    file=f.read()\n",
    "#empty token list=\n",
    "all_tokens=[]\n",
    "eq=[\"=\"]\n",
    "op=['+','-','*','<','>','/','%','!']\n",
    "do=['++','--','+=','*=','/=']\n",
    "sym=[';','#','\\t']\n",
    "punct=[',',':','}','{','[',']','(',')']\n",
    "result = []\n",
    "all_tokens = []\n",
    "num=['0','1','2','3','4','5','6','7','8','9']\n",
    "le=''\n",
    "sp=' '\n",
    "c = 1\n",
    "key=op+sym+eq+punct+do\n",
    "string=\"'m'a%==b\\n65.75.ib\\n'dfgsdfg'\\na<<<==b \\nopoopo\"\n",
    "string=file\n",
    "for a,i in enumerate(string):\n",
    "    if i == '\\n':\n",
    "        le+=i\n",
    "        toke = token(le)\n",
    "        n=toke.tokenizer()\n",
    "        for t in n:\n",
    "            result.append(t)\n",
    "        result.append(c)\n",
    "        all_tokens.append(result)\n",
    "        print(result)\n",
    "        c+=1\n",
    "        result=[]\n",
    "        le=le.strip()#for newline\n",
    "    try:\n",
    "        if i !=sp and i not in sym:#checking and appending into le \n",
    "            if i in op:\n",
    "                try:\n",
    "                    if string[a+1] in eq:\n",
    "                        le=i+string[a+1]\n",
    "                    elif i==string[a+1] and i!=string[a-1]:\n",
    "                        le=i+string[a+1]\n",
    "                    elif i==string[a+1] and i==string[a-1]:\n",
    "                        le+=i\n",
    "                    elif i==string[a-1]:\n",
    "                        pass\n",
    "                    \n",
    "                    else:\n",
    "                        le+=i\n",
    "                except:\n",
    "                    le+=i\n",
    "            elif i in eq:\n",
    "                try:\n",
    "                    if string[a+1] in op or string[a-1] in op :\n",
    "                        pass\n",
    "                    elif i==string[a+1] and i!=string[a-1]:\n",
    "                        le=i+string[a+1]\n",
    "                    elif i==string[a-1] and i==string[a+1]:\n",
    "                        le+=i\n",
    "                    elif i==string[a-1] or string[a-2]in op:\n",
    "                        le+=i\n",
    "                    elif i==string[a-1] or string[a-2]not in op:\n",
    "                        pass\n",
    "                    else:\n",
    "                        le+=i\n",
    "                except: \n",
    "                    le+=i\n",
    "            else:\n",
    "                le+=i\n",
    "    except IndexError:\n",
    "        print(\"END OF FILE\")\n",
    "    if(a+1<=len(string)):\n",
    "        try:\n",
    "            if string[a+1] == sp or string[a+1] in key or le in key or string[a+1] == '\\n' or (string[a+1]==\".\" and i not in no and i !='.'):#checking the next word(breaker)\n",
    "                \n",
    "                if le != '':\n",
    "                    if '\\n' in le:\n",
    "                        \n",
    "                        le=le.strip()\n",
    "                        #print(le.replace('\\n',''))\n",
    "                        toke = token(le)\n",
    "                        n=toke.tokenizer()\n",
    "                        for t in n:\n",
    "                            result.append(t)\n",
    "                        result.append(c)    \n",
    "                        all_tokens.append(result)\n",
    "                        print(result)\n",
    "                        result = []             \n",
    "                        le=''\n",
    "                    else:  \n",
    "                        toke = token(le)\n",
    "                        n=toke.tokenizer()\n",
    "                        for t in n:\n",
    "                            result.append(t)\n",
    "                        result.append(c)     \n",
    "                        all_tokens.append(result)\n",
    "                        print(result)\n",
    "                        result = []               \n",
    "                        le=''\n",
    "           \n",
    "        except:\n",
    "            print(\"end of fileii\")\n",
    "\n",
    "print(all_tokens)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my name is sadaf.\n"
     ]
    }
   ],
   "source": [
    "n=\"my name is sadaf.\"\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['65', 'INTEGER CONSTANT', 1]\n",
      "['.75', 'FLOAT CONSTANT', 1]\n",
      "['.inn', 'Invalid Token', 1]\n",
      "[\"'m'\", 'CHAR CONSTANT', 1]\n",
      "end of fileii\n",
      "[['65', 'INTEGER CONSTANT', 1], ['.75', 'FLOAT CONSTANT', 1], ['.inn', 'Invalid Token', 1], [\"'m'\", 'CHAR CONSTANT', 1]]\n"
     ]
    }
   ],
   "source": [
    "with open(\"SAF1.txt\",\"r\")as f:\n",
    "    file=f.read()\n",
    "#empty token list=\n",
    "all_tokens=[]\n",
    "eq=[\"=\"]\n",
    "op=['+','-','*','<','>','/','%','!']\n",
    "do=['++','--','+=','*=','/=']\n",
    "sym=[';','#','\\t']\n",
    "punct=[',',':','}','{','[',']','(',')']\n",
    "result = []\n",
    "all_tokens = []\n",
    "num=['0','1','2','3','4','5','6','7','8','9']\n",
    "le=''\n",
    "sp=' '\n",
    "c = 1\n",
    "key=op+sym+eq+punct+do\n",
    "string=\"65.75.inn \\'m\\' \"\n",
    "for a,i in enumerate(string):\n",
    "    if i == '\\n':\n",
    "        le+=i\n",
    "        #temp=le\n",
    "        toke = token(le)\n",
    "        n=toke.tokenizer()\n",
    "        for t in n:\n",
    "            result.append(t)\n",
    "        result.append(c)\n",
    "        all_tokens.append(result)\n",
    "        print(result)\n",
    "        c+=1\n",
    "        result=[]\n",
    "        le=le.strip()#for newline\n",
    "        #temp=le\n",
    "    try:\n",
    "        if i !=sp and i not in sym:#checking and appending into le \n",
    "            if i in op:\n",
    "                try:\n",
    "                    if string[a+1] in eq:\n",
    "                        le=i+string[a+1]\n",
    "                    elif i==string[a+1] and i!=string[a-1]:\n",
    "                        le=i+string[a+1]\n",
    "                    elif i==string[a+1] and i==string[a-1]:\n",
    "                        le+=i\n",
    "                    elif i==string[a-1]:\n",
    "                        pass\n",
    "                    \n",
    "                    else:\n",
    "                        le+=i\n",
    "                except:\n",
    "                    le+=i\n",
    "            elif i in eq:\n",
    "                try:\n",
    "                    if string[a+1] in op or string[a-1] in op :\n",
    "                        pass\n",
    "                    elif i==string[a+1] and i!=string[a-1]:\n",
    "                        le=i+string[a+1]\n",
    "                    elif i==string[a-1] and i==string[a+1]:\n",
    "                        le+=i\n",
    "                    elif i==string[a-1] or string[a-2]in op:\n",
    "                        le+=i\n",
    "                    elif i==string[a-1] or string[a-2]not in op:\n",
    "                        pass\n",
    "                    else:\n",
    "                        le+=i\n",
    "                except: \n",
    "                    le+=i\n",
    "            else:\n",
    "                le+=i\n",
    "    except IndexError:\n",
    "        print(\"END OF FILE\")\n",
    "    if(a+1<=len(string)):\n",
    "        try:\n",
    "            if string[a+1] == sp or string[a+1] in key or le in key or string[a+1] == '\\n' or string[a+1]=='.':#checking the next word(breaker)\n",
    "                \n",
    "                if le != '':\n",
    "                    if '\\n' in le:\n",
    "                        \n",
    "                        le=le.strip()\n",
    "                        #print(le.replace('\\n',''))\n",
    "                        toke = token(le)\n",
    "                        n=toke.tokenizer()\n",
    "                        for t in n:\n",
    "                            result.append(t)\n",
    "                        result.append(c)    \n",
    "                        all_tokens.append(result)\n",
    "                        print(result)\n",
    "                        result = [] \n",
    "                        #temp=le\n",
    "                        le=''\n",
    "                    elif '.' in le:\n",
    "                        \n",
    "                        le=le.strip()\n",
    "                        #print(le.replace('\\n',''))\n",
    "                        toke = token(le)\n",
    "                        n=toke.tokenizer()\n",
    "                        for t in n:\n",
    "                            result.append(t)\n",
    "                        result.append(c)    \n",
    "                        all_tokens.append(result)\n",
    "                        print(result)\n",
    "                        result = [] \n",
    "                        #temp=le\n",
    "                        le=''\n",
    "                    else:  \n",
    "                        toke = token(le)\n",
    "                        n=toke.tokenizer()\n",
    "                        for t in n:\n",
    "                            result.append(t)\n",
    "                        result.append(c)     \n",
    "                        all_tokens.append(result)\n",
    "                        print(result)\n",
    "                        result = []               \n",
    "                        #temp=le\n",
    "                        le=''\n",
    "           \n",
    "        except:\n",
    "            print(\"end of fileii\")\n",
    "\n",
    "print(all_tokens)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asd\n",
      "+\n",
      "4.5\n",
      ".6\n",
      "end of file\n"
     ]
    }
   ],
   "source": [
    "string=\"a+=b\"\n",
    "eq=[\"=\"]\n",
    "op=['+','-','*','<','>','/','|','band','bor','!']\n",
    "do=['++','--','+=','*=','/=']\n",
    "sym=[';','#','\\t']\n",
    "punct=[',',':','}','{','[',']','(',')']\n",
    "no=['1','2','3','4','5','6','7','8','9','0']\n",
    "result = []\n",
    "all_tokens = []\n",
    "num=['0','1','2','3','4','5','6','7','8','9']\n",
    "le=''\n",
    "sp=' '\n",
    "string=\"asd + 4.5.6 \"\n",
    "c = 1\n",
    "for a,i in enumerate(string):\n",
    "    if i != sp and i not in sym :\n",
    "        \n",
    "        if i in op:\n",
    "            \n",
    "            try:\n",
    "                if string[a+1] in eq:\n",
    "                    le=i+string[a+1]\n",
    "                elif i==string[a+1] and i!=string[a-1]:\n",
    "                    le=i+string[a+1]\n",
    "                elif i==string[a+1] and i==string[a-1]:\n",
    "                    le+=i\n",
    "                elif i==string[a-1]:\n",
    "                    pass\n",
    "                    \n",
    "                else:\n",
    "                    le+=i\n",
    "            except:\n",
    "                le+=i\n",
    "        elif i==\".\" and string[a+1]  in no:\n",
    "            le+=i\n",
    "            #le.replace('.','')\n",
    "            le= le.strip()\n",
    "        else:\n",
    "            le+=i\n",
    "            #le.replace('.','')\n",
    "    if(a+1<=len(string)):\n",
    "        try:\n",
    "            if string[a+1] == sp or string[a+1] in key or le in key or string[a+1] == '\\n':\n",
    "                #checking the next word(breaker)\n",
    "                if le != '':\n",
    "                    print(le.replace('\\n','\\n\\r'))\n",
    "                    le=\"\"\n",
    "            elif i not in num and string[a+1] == \".\"and string[a+2] not in num:#(breakforalpha)\n",
    "                if le != '':\n",
    "                    print(le.replace('\\n','\\n\\r'))\n",
    "                    le=\"\"\n",
    "        \n",
    "            elif i in num and string[a+1]=='.' and string[a+2] in num and \".\" in le:\n",
    "                if le != '':\n",
    "                    print(le.replace('\\n','\\n\\r'))\n",
    "                    le=\"\"\n",
    "            \n",
    "                \n",
    "            \n",
    "        except:\n",
    "            print(\"end of file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[]']\n"
     ]
    }
   ],
   "source": [
    "a=str(all_tokens)\n",
    "#writting the tokens in the file\n",
    "with open(\"tokens.txt\",'w')as tk:\n",
    "    tk.writelines(a)\n",
    "#reading\n",
    "with open(\"tokens.txt\",'r')as tk:\n",
    "    to=tk.readlines()\n",
    "print(to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sdfg\n",
      "wer\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le='sdfg'\n",
    "temp=le\n",
    "le=''\n",
    "print(temp)\n",
    "le='wer'\n",
    "temp=le\n",
    "print(temp)\n",
    "\".\" in le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
